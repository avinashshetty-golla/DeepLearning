{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b214609-8b89-4d8c-8c41-c344071c7665",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages/transformers/utils/generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (593 > 512). Running this sequence through the model will result in indexing errors\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2320/2320 [09:01<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Average Training Loss: 5.914451896938784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2320/2320 [09:03<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Average Training Loss: 5.288564804607424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2320/2320 [09:03<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3, Average Training Loss: 4.688990122079849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 15875/15875 [01:44<00:00, 152.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final F1 Score: 0.005956339899499583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForQuestionAnswering\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW\n",
    "import jiwer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Set device to GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "def load_data_from_json(filepath):\n",
    "    with open(filepath, 'rb') as file:\n",
    "        dataset = json.load(file)\n",
    "    passages, questions, answers = [], [], []\n",
    "\n",
    "    for item in dataset['data']:\n",
    "        for paragraph in item['paragraphs']:\n",
    "            context = paragraph['context'].lower()\n",
    "            for qa in paragraph['qas']:\n",
    "                question = qa['question'].lower()\n",
    "                for answer in qa['answers']:\n",
    "                    passages.append(context)\n",
    "                    questions.append(question)\n",
    "                    answers.append(answer)\n",
    "\n",
    "    return passages, questions, answers\n",
    "\n",
    "def calculate_answer_bounds(answers, passages):\n",
    "    for ans, passage in zip(answers, passages):\n",
    "        ans['text'] = ans['text'].lower()\n",
    "        ans['answer_end'] = ans['answer_start'] + len(ans['text'])\n",
    "\n",
    "def process_and_tokenize(passages, questions, answers, tokenizer, max_len):\n",
    "    tokenized_data = tokenizer(questions, passages, max_length=max_len, padding=\"max_length\", truncation=True)\n",
    "    start_positions, end_positions = [], []\n",
    "\n",
    "    for i, (answer, passage) in enumerate(zip(answers, passages)):\n",
    "        tokens = tokenized_data['input_ids'][i]\n",
    "        start, end = answer['answer_start'], answer['answer_end']\n",
    "        \n",
    "        start_ids = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(passage[:start]))\n",
    "        end_ids = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(passage[:end]))\n",
    "\n",
    "        if not start_ids or not end_ids:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "            continue\n",
    "\n",
    "        midpoint = (start_ids[0] + end_ids[0]) // 2\n",
    "        start_segment = max(0, min(midpoint - max_len // 2, len(tokens) - max_len))\n",
    "\n",
    "        start_positions.append(start - start_segment)\n",
    "        end_positions.append(end - start_segment)\n",
    "\n",
    "    tokenized_data.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "    return tokenized_data\n",
    "\n",
    "class CustomQADataset(Dataset):\n",
    "    def __init__(self, tokenized_data):\n",
    "        self.data = tokenized_data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data['input_ids'])\n",
    "\n",
    "# Load and process data for training and validation\n",
    "train_passages, train_questions, train_answers = load_data_from_json('spoken_train-v1.1.json')\n",
    "val_passages, val_questions, val_answers = load_data_from_json('spoken_test-v1.1.json')\n",
    "\n",
    "# Calculate boundaries for answers in training and validation sets\n",
    "calculate_answer_bounds(train_answers, train_passages)\n",
    "calculate_answer_bounds(val_answers, val_passages)\n",
    "\n",
    "MAX_TOKEN_LENGTH = 512\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Tokenize data\n",
    "train_encodings = process_and_tokenize(train_passages, train_questions, train_answers, tokenizer, MAX_TOKEN_LENGTH)\n",
    "val_encodings = process_and_tokenize(val_passages, val_questions, val_answers, tokenizer, MAX_TOKEN_LENGTH)\n",
    "\n",
    "train_dataset = CustomQADataset(train_encodings)\n",
    "val_dataset = CustomQADataset(val_encodings)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1)\n",
    "\n",
    "# Load model and optimizer\n",
    "model = DistilBertForQuestionAnswering.from_pretrained(MODEL_NAME).to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training function\n",
    "def train(model, data_loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(data_loader, desc='Training'):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_pos = batch['start_positions'].to(device)\n",
    "        end_pos = batch['end_positions'].to(device)\n",
    "\n",
    "        output = model(input_ids=input_ids, attention_mask=attention_mask, start_positions=start_pos, end_positions=end_pos)\n",
    "        loss = output.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "# Validation function with F1 calculation\n",
    "def validate(model, data_loader):\n",
    "    model.eval()\n",
    "    true_labels, predictions = [], []\n",
    "\n",
    "    for batch in tqdm(data_loader, desc='Validation'):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        true_start = batch['start_positions'].to(device)\n",
    "        true_end = batch['end_positions'].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        pred_start = torch.argmax(outputs.start_logits, dim=1)\n",
    "        pred_end = torch.argmax(outputs.end_logits, dim=1)\n",
    "\n",
    "        predictions.extend([(pred_start[i].item(), pred_end[i].item()) for i in range(len(true_start))])\n",
    "        true_labels.extend([(true_start[i].item(), true_end[i].item()) for i in range(len(true_start))])\n",
    "\n",
    "    # Compute F1 score\n",
    "    true_start_labels = [x[0] for x in true_labels]\n",
    "    true_end_labels = [x[1] for x in true_labels]\n",
    "    pred_start_labels = [x[0] for x in predictions]\n",
    "    pred_end_labels = [x[1] for x in predictions]\n",
    "\n",
    "    f1_start = f1_score(true_start_labels, pred_start_labels, average=\"macro\")\n",
    "    f1_end = f1_score(true_end_labels, pred_end_labels, average=\"macro\")\n",
    "    overall_f1 = (f1_start + f1_end) / 2\n",
    "\n",
    "    return overall_f1\n",
    "\n",
    "# Training loop for 3 epochs\n",
    "NUM_EPOCHS = 3\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    avg_train_loss = train(model, train_loader, optimizer)\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Average Training Loss: {avg_train_loss}\")\n",
    "\n",
    "# Calculate final F1 score on validation set\n",
    "final_f1_score = validate(model, val_loader)\n",
    "print(f\"Final F1 Score: {final_f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637d5531-08d7-477b-bc74-ba9e4fbc2834",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
