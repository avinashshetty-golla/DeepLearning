{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3d30121-4e5a-4c1a-a622-8e59b51e21c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (593 > 512). Running this sequence through the model will result in indexing errors\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Training: 100%|██████████| 2320/2320 [09:03<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Train Loss: 4.1328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2320/2320 [09:04<00:00,  4.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Train Loss: 3.0676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 18/2320 [00:04<09:01,  4.25it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Training: 100%|██████████| 2320/2320 [09:04<00:00,  4.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Train Loss: 2.3063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 15875/15875 [01:48<00:00, 146.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Evaluation - WER: 2.0521, F1 Score: 0.6840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForQuestionAnswering\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW\n",
    "import jiwer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Data parsing function\n",
    "def parse_data(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    contexts, questions, answers = [], [], []\n",
    "\n",
    "    for item in data['data']:\n",
    "        for paragraph in item['paragraphs']:\n",
    "            context_text = paragraph['context']\n",
    "            for qna in paragraph['qas']:\n",
    "                question_text = qna['question']\n",
    "                for answer in qna['answers']:\n",
    "                    contexts.append(context_text.lower())\n",
    "                    questions.append(question_text.lower())\n",
    "                    answers.append(answer)\n",
    "    \n",
    "    return contexts, questions, answers\n",
    "\n",
    "# Function to compute answer end positions\n",
    "def compute_end_positions(answers, contexts):\n",
    "    for answer, context in zip(answers, contexts):\n",
    "        answer['text'] = answer['text'].lower()\n",
    "        answer['end'] = answer['answer_start'] + len(answer['text'])\n",
    "\n",
    "# Prepare inputs with tokenized start and end positions\n",
    "def prepare_inputs(contexts, questions, answers, tokenizer, max_len):\n",
    "    inputs = tokenizer(questions, contexts, max_length=max_len, padding=\"max_length\", truncation=True)\n",
    "    start_positions, end_positions = [], []\n",
    "\n",
    "    for i, (answer, context) in enumerate(zip(answers, contexts)):\n",
    "        answer_start = answer['answer_start']\n",
    "        answer_end = answer['end']\n",
    "        \n",
    "        token_start = tokenizer.encode(context[:answer_start], add_special_tokens=False)\n",
    "        token_end = tokenizer.encode(context[:answer_end], add_special_tokens=False)\n",
    "\n",
    "        start_positions.append(len(token_start))\n",
    "        end_positions.append(len(token_end) - 1)\n",
    "\n",
    "    inputs.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "    return inputs\n",
    "\n",
    "# Custom dataset class\n",
    "class QuestionAnsweringDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {key: torch.tensor(val[index]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "# Load data and compute end positions\n",
    "train_contexts, train_questions, train_answers = parse_data('spoken_train-v1.1.json')\n",
    "valid_contexts, valid_questions, valid_answers = parse_data('spoken_test-v1.1.json')\n",
    "\n",
    "compute_end_positions(train_answers, train_contexts)\n",
    "compute_end_positions(valid_answers, valid_contexts)\n",
    "\n",
    "# Tokenize data\n",
    "MAX_LEN = 512\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "\n",
    "train_data = prepare_inputs(train_contexts, train_questions, train_answers, tokenizer, MAX_LEN)\n",
    "valid_data = prepare_inputs(valid_contexts, valid_questions, valid_answers, tokenizer, MAX_LEN)\n",
    "\n",
    "train_dataset = QuestionAnsweringDataset(train_data)\n",
    "valid_dataset = QuestionAnsweringDataset(valid_data)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=1)\n",
    "\n",
    "# Model and optimizer\n",
    "qa_model = DistilBertForQuestionAnswering.from_pretrained(MODEL_NAME).to(device)\n",
    "optimizer = AdamW(qa_model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training function\n",
    "def train_one_epoch(model, loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(loader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        start_pos = batch[\"start_positions\"].to(device)\n",
    "        end_pos = batch[\"end_positions\"].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, start_positions=start_pos, end_positions=end_pos)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Evaluation function with F1 score calculation\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    word_error_rates = []\n",
    "    all_true_spans = []\n",
    "    all_pred_spans = []\n",
    "\n",
    "    for batch in tqdm(loader, desc=\"Evaluating\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        true_start = batch[\"start_positions\"].to(device)\n",
    "        true_end = batch[\"end_positions\"].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        start_preds = torch.argmax(outputs.start_logits, dim=1)\n",
    "        end_preds = torch.argmax(outputs.end_logits, dim=1)\n",
    "\n",
    "        for i in range(len(true_start)):\n",
    "            pred_ans = tokenizer.decode(input_ids[i][start_preds[i]:end_preds[i]+1])\n",
    "            actual_ans = tokenizer.decode(input_ids[i][true_start[i]:true_end[i]+1])\n",
    "\n",
    "            if actual_ans.strip():\n",
    "                wer = jiwer.wer(actual_ans, pred_ans)\n",
    "                word_error_rates.append(wer)\n",
    "\n",
    "                # Token-level F1 calculation\n",
    "                true_tokens = set(tokenizer.encode(actual_ans, add_special_tokens=False))\n",
    "                pred_tokens = set(tokenizer.encode(pred_ans, add_special_tokens=False))\n",
    "                all_true_spans.append(true_tokens)\n",
    "                all_pred_spans.append(pred_tokens)\n",
    "\n",
    "    avg_wer = sum(word_error_rates) / len(word_error_rates) if word_error_rates else 0.0\n",
    "\n",
    "    # Calculate token-level F1 for the entire program\n",
    "    all_true_tokens = set().union(*all_true_spans)\n",
    "    all_pred_tokens = set().union(*all_pred_spans)\n",
    "    true_positives = len(all_true_tokens & all_pred_tokens)\n",
    "    precision = true_positives / len(all_pred_tokens) if all_pred_tokens else 0\n",
    "    recall = true_positives / len(all_true_tokens) if all_true_tokens else 0\n",
    "    final_f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) else 0\n",
    "\n",
    "    return avg_wer, final_f1_score\n",
    "\n",
    "# Training loop with 3 epochs\n",
    "EPOCHS = 3\n",
    "for epoch in range(EPOCHS):\n",
    "    avg_train_loss = train_one_epoch(qa_model, train_loader, optimizer)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "# Final evaluation with F1 score\n",
    "wer, final_f1_score = evaluate(qa_model, valid_loader)\n",
    "print(f\"Final Evaluation - WER: {wer:.4f}, F1 Score: {final_f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc096da8-93b2-4474-8b4e-4215971d4ab1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
